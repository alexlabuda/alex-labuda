---
title: "Bootstrapped Resampling Regression & Revenue Forecasting"
subtitle: ""
excerpt: "We'll analyze regression coefficients with bootstrapped resamples, visualize most important features and forecast future revenue"
date: 2022-11-28
author: "Alex Labuda"
draft: false
thumbnail_left: true # for list-sidebar only
show_author_byline: true
images: 
series:
tags: ["machine learning", "regression", "feature importance", "time-series forecasting"]
categories:
layout: single
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.width = 8, fig.height = 5)
library(tidyverse)
theme_set(theme_minimal())
```

In this post, I'll begin by visualizing marketing related time-series data which includes revenue and spend for several direct marketing channels.

I will then fit a simple regression model and examine the effect of each channel on revenue.

In order to better understand the reliability of each coefficient, I will fit many bootstrapped resamples to develop a more robust estimate of each coefficient.

Lastly, I will create a forecasting model to predict future revenue. Enjoy!

```{r}
library(modeltime)
library(timetk)
library(lubridate)

interactive <- FALSE
```

# The Data

```{r}
df <- 
  read_csv(
    file = "data/marketing_data.csv"
  ) %>% 
  mutate(
    date = mdy(date)
  )
```

# Visualize the Data

First lets visualize our target variable, `revenue`
```{r}
df %>% 
  ggplot(aes(date, revenue)) +
  geom_line() +
  geom_smooth(se = FALSE, color = "darkred") +
  labs(title = "Revenue",
       x = "")
```

# Fit: Simple Regression Model

First we'll start by fitting a simple regression model to the data. This is just the grand mean of our data.

```{r}
df %>%
    plot_time_series_regression(
        .date_var     = date,
        .formula      = revenue ~ as.numeric(date),
        .interactive  = interactive,
        .show_summary = FALSE
    )
```

We can make a more accurate model by using month of year as independent variables for our model

```{r}
df %>%
    plot_time_series_regression(
        .date_var     = date,
        .formula      = revenue ~ as.numeric(date) + month(date, label = TRUE),
        .interactive  = interactive,
        .show_summary = FALSE
    )
```

We can also introduce our spend variables as input to our model to study each channels effect on revenue

```{r}
df %>%
    plot_time_series_regression(
        .date_var     = date,
        .formula      = revenue ~ as.numeric(date) + month(date, label = TRUE) + tv_spend +
          billboard_spend + print_spend + search_spend + facebook_spend + competitor_sales,
        .interactive  = interactive,
        .show_summary = FALSE
    )
```

# A Simple Linear Model


Now lets take a look at our simple model output that includes direct marketing features and examine their effect of revenue

We can see that:
- TV spend, print spend, competitor sales and Facebook spend are statistically significant
- Print spend has the largest positive effect at increasing revenue

```{r}
# forcing intercept to 0 to just show the gap from 0 instead of a base
revenue_fit <- lm(revenue ~ as.numeric(date) + tv_spend + billboard_spend + print_spend + 
                    search_spend + facebook_spend + competitor_sales, data = df)
summary(revenue_fit)
```

## Viz: SLM coefficients

We can plot the coefficient and confidence intervals to make it easier to see the reliability of each estimate

- Print spend has the largest positive coefficient, but also the widest confidence interval
- Our model is very confident in the estimated effect of competitor sales
```{r}
library(broom)
library(dotwhisker)

tidy(revenue_fit) %>% 
  filter(p.value < 0.1) %>% 
  mutate(
         term = fct_reorder(term, -estimate)) %>% 
  dwplot(
    vars_order = levels(.$term),
    dot_args = list(size = 3, color = "darkred"),
  whisker_args = list(color = "darkred", alpha = 0.75)
  ) +
  labs(x = "Coefficient by channel", y = NULL, title = "Simple Linear Model Coefficients", subtitle = "95% Confidence Intervals")
```

# Fit: Bootstrap Resampling 

## How reliable are our coefficients?

We can fit many bootstrapped resampled models to determine the stability of our coefficient estimates. Essentially this is fitting many small models to examine the variation of each channel's coefficient.

- By default `reg_intervals` uses 1,001 bootstrap samples for t-intervals and 2,001 for percentile intervals.

```{r}
library(rsample)

revenue_intervals <-
  reg_intervals(revenue ~ as.numeric(date) + tv_spend + billboard_spend + print_spend + 
                    search_spend + facebook_spend + competitor_sales,
                data = df, keep_reps = TRUE)

revenue_intervals
```

## Viz: Bootstrapped Resampled Coefficients

### Crossbar chart

- Here we can see the wide confidence interval surround `search_spend`. 
- This makes sense since the coefficient for this channel in our first linear model was not statistically significant. 
```{r}
revenue_intervals %>%
  filter(!term == "as.numeric(date)") %>% 
    mutate(
        term = fct_reorder(term, .estimate)
    ) %>%
    ggplot(aes(.estimate, term)) +
    geom_crossbar(aes(xmin = .lower, xmax = .upper),
                  color = "darkred", alpha = 0.8) +
  labs(x = "Coefficient by channel", 
       y = NULL, 
       title = "Bootstrapped Resampled Model Coefficients", 
       subtitle = "95% Confidence Intervals")
```

# Time-Series Forecasting

Lets build our forecasting model
```{r}
df_ts <- 
  df %>% 
  select(date, revenue)

df_ts %>% 
  ggplot(aes(date, revenue)) +
  geom_line() +
  labs(title = "Revenue",
       x = "")
```

## Training & Testing Splits

First I'll split the data into training and testing sets

```{r}
splits <- 
  df_ts %>% 
  time_series_split(assess = "12 months", cumulative = TRUE)

splits %>% 
  tk_time_series_cv_plan() %>% 
  plot_time_series_cv_plan(date, revenue, .interactive = interactive) +
  labs(title = "Timeseries - Training / Testing Split")
```

## Modeling

Now we can begin the recipe for our model. After baking, we can see what the output looks like.

```{r}
library(tidymodels)
recipe_spec_timeseries <- 
  recipe(revenue ~., data = training(splits)) %>%
    step_timeseries_signature(date) 

bake(prep(recipe_spec_timeseries), new_data = training(splits))
```

### Preprocessing Steps

Now we'll add a few additional preprocessing steps to our recipe:
- Convert a date column into a Fourier series
- Remove date
- Normalize the inputs
- one-hot encoding (add dummies)
```{r}
recipe_spec_final <- recipe_spec_timeseries %>%
    step_fourier(date, period = 365, K = 1) %>%
    step_rm(date) %>%
    step_rm(contains("iso"), contains("minute"), contains("hour"),
            contains("am.pm"), contains("xts")) %>%
    step_normalize(contains("index.num"), date_year) %>%
    step_dummy(contains("lbl"), one_hot = TRUE) 

juice(prep(recipe_spec_final))
```

## Model Specs

Now we can fit a simple linear model
```{r}
model_spec_lm <- linear_reg(mode = "regression") %>%
    set_engine("lm")
```

## Workflow

We can add our recipe and model to a workflow
```{r}
workflow_lm <- workflow() %>%
    add_recipe(recipe_spec_final) %>%
    add_model(model_spec_lm)

workflow_lm
```

## Fit our Model
```{r}
workflow_fit_lm <- workflow_lm %>% fit(data = training(splits))
```


```{r}
model_table <- modeltime_table(
  workflow_fit_lm
) 

calibration_table <- model_table %>%
  modeltime_calibrate(testing(splits))
```

## Forcasting Results

Lets visualize our models prediction accuracy
```{r}
calibration_table %>%
  modeltime_forecast(actual_data = df_ts) %>%
  plot_modeltime_forecast(.interactive = FALSE) +
  labs(
    y = "Revenue",
    title = "Revenue",
    subtitle = "Actual vs Predicted"
  ) +
  theme(legend.position = "none")
```

Thanks for reading, I hope you found this post useful.






