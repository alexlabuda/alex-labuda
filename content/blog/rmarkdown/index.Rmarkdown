---
title: "Predicting chocolate bar ratings"
subtitle: ""
excerpt: "predicting chocolate bar ratings with tidytext"
date: 2022-11-26
author: "Alex Labuda"
draft: false
thumbnail_left: true # for list-sidebar only
show_author_byline: true
images: 
series:
tags: ["machine learning"]
categories:
layout: single
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 8, fig.height = 5)
library(tidyverse)
theme_set(theme_minimal())
```

*This is a work in progress blog post where we will try to predict chocolate bar ratings based on various features*

## The Data

The data includes various chocolate bars and their corresponding ratings

```{r message=FALSE, warning=FALSE}
tuesdata <- tidytuesdayR::tt_load(2022, week = 3)
chocolate <- tuesdata$chocolate
```

## Explore data

First we'll start by exploring the data

Exploratory data analysis (EDA) is an [important part of the modeling process](https://www.tmwr.org/software-modeling.html#model-phases).

```{r}
chocolate %>% 
  ggplot(aes(rating)) +
  geom_histogram(bins = 12)
```

Next I will replace missing values in ingredients with the most common ingredient and remove `%` from cocoa_percent column
```{r}
chocolate <- 
  chocolate %>% 
  mutate(
    ingredients = replace_na(ingredients, "3- B,S,C")
    )

chocolate <- 
  chocolate %>% 
  mutate(cocoa_percent = as.integer(str_remove_all(cocoa_percent, "%")))
```

Next I bin all infrequent `ingredients`, `company_location` and `country_of_bean_origin` in "Other"category
```{r}
chocolate <- 
  chocolate %>% 
  mutate(
    ingredients = ifelse(!ingredients %in% c("3- B,S,C", "2- B,S"),
                         "Other",
                         ingredients),
    company_location = ifelse(!company_location %in% c("U.S.A.", "Canada", "France",
                                                       "U.K.", "Italy", "Belgium"),
                              "Other",
                              company_location),
    country_of_bean_origin = ifelse(!country_of_bean_origin %in% c("Venezuela", "Peru",
                                                                   "Dominican Republic",
                                                                   "Ecuador", "Madagascar",
                                                                   "Blend", "Nicaragua"),
                                    "Other",
                                    country_of_bean_origin)
  )
```

Now we can preview out data with `skimr::skim()`
```{r}
skimr::skim(chocolate)
```

## Feature Engineering
```{r}
library(tidytext)

tidy_chocolate <- 
  chocolate %>% 
  # puts each word on it's own row
  unnest_tokens(word, most_memorable_characteristics)

tidy_chocolate %>% 
  count(word, sort = TRUE)

```

### Rating by word count

```{r}
tidy_chocolate %>% 
  group_by(word) %>% 
  summarise(n = n(),
            rating = mean(rating)) %>% 
  ggplot(aes(n, rating)) +
  geom_hline(yintercept = mean(chocolate$rating),
             lty = 2, color = "gray50", size = 1.25) +
  geom_point(color = "midnightblue", alpha = 0.7) +
  geom_text(aes(label = word), 
            check_overlap = TRUE, vjust = "top", hjust = "left") + 
  scale_x_log10()
```

## Build models

Let's consider how to [spend our data budget](https://www.tmwr.org/splitting.html):

-   create training and testing sets
-   create resampling folds from the *training* set

```{r}
library(tidymodels)

set.seed(123)
choco_split <- initial_split(chocolate, strata = rating)
choco_train <- training(choco_split)
choco_test <- testing(choco_split)

set.seed(234)
choco_folds <- vfold_cv(choco_train, strata = rating)
choco_folds
```

## Preprocessing

Let's set up our preprocessing

### Recipe

```{r}
library(textrecipes)

choco_recipe <- 
  # recipe for the estimator
  recipe(rating ~ most_memorable_characteristics
         , data = choco_train) %>%
  # steps to build the estimator
  step_tokenize(most_memorable_characteristics) %>% 
  step_tokenfilter(most_memorable_characteristics, max_tokens = 100) %>% 
  step_tf(most_memorable_characteristics) 


# Next steps are not necessary here, but are good to take a look at whats going on

# prep on recipe is analogous with fit on a model
# now the steps say [trained]
prep(choco_recipe)

# bake for a recipe is like predict for a model
prep(choco_recipe) %>% 
  bake(new_data = NULL)

```

### Specs

Let's create a [**model specification**](https://www.tmwr.org/models.html) for each model we want to try:

```{r}
ranger_spec <-
  # the algorithm
  rand_forest(trees = 500) %>%
  # engine (computation to fit the model (keras, spark, etc.))
  set_engine("ranger") %>% # range is the default
  # describes the modeling problem you're working with (regression, classification, etc.)
  set_mode("regression")

ranger_spec

svm_spec <- 
  svm_linear() %>% 
  set_engine("LiblineaR") %>% 
  set_mode("regression")

svm_spec
```

To set up your modeling code, consider using the [parsnip addin](https://parsnip.tidymodels.org/reference/parsnip_addin.html) or the [usemodels](https://usemodels.tidymodels.org/) package.

Now let's build a [**model workflow**](https://www.tmwr.org/workflows.html) combining each model specification with a data preprocessor:

### WorkFlow

```{r}
# we're using a workflow bc we have a more complex model preprocess
ranger_wf <- workflow(choco_recipe, ranger_spec)
svm_wf <- workflow(choco_recipe, svm_spec)
```

If your feature engineering needs are more complex than provided by a formula like `sex ~ .`, use a [recipe](https://www.tidymodels.org/start/recipes/). [Read more about feature engineering with recipes](https://www.tmwr.org/recipes.html) to learn how they work.

## Evaluate models

These models have no tuning parameters so we can evaluate them as they are. [Learn about tuning hyperparameters here.](https://www.tidymodels.org/start/tuning/)

```{r}
doParallel::registerDoParallel()
contrl_preds <- control_resamples(save_pred = TRUE) # keep predictions

svm_rs <- fit_resamples(
  svm_wf,
  resamples = choco_folds,
  control = contrl_preds
)

ranger_rs <- fit_resamples(
  ranger_wf,
  resamples = choco_folds,
  control = contrl_preds
)
```

How did these two models compare?

```{r}
collect_metrics(svm_rs)
collect_metrics(ranger_rs)
```

We can visualize these results:

```{r}
bind_rows(
  collect_predictions(svm_rs) %>%
    mutate(mod = "SVM"),
  collect_predictions(ranger_rs) %>%
    mutate(mod = "ranger")
) %>%
  ggplot(aes(rating, .pred, color = id)) +
  geom_abline(lty = 2, color = "gray50", size = 1.2) + 
  geom_jitter(width = 0.5, alpha = 0.5) +
  facet_wrap(vars(mod)) + 
  coord_fixed()
```

These models perform very similarly, so perhaps we would choose the simpler, linear model. The function `last_fit()` *fits* one final time on the training data and *evaluates* on the testing data. This is the first time we have used the testing data.

```{r}
final_fitted <- last_fit(svm_wf, choco_split)
collect_metrics(final_fitted)  ## metrics evaluated on the *testing* data
```

This object contains a fitted workflow that we can use for prediction.

```{r}
final_wf <- extract_workflow(final_fitted)
predict(final_wf, choco_test[55,])
```

You can save this fitted `final_wf` object to use later with new data, for example with `readr::write_rds()`.

```{r}
extract_workflow(final_fitted) %>% 
  tidy() %>% 
  filter(term != "Bias") %>% 
  group_by(estimate > 0) %>% 
  slice_max(abs(estimate), n = 10) %>% 
  ungroup() %>% 
  mutate(term = str_remove(term, "tf_most_memorable_characteristics_")) %>% 
  ggplot(aes(estimate, fct_reorder(term, estimate), fill = estimate > 0)) +
  geom_col(alpha = 0.8)
```
